{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "#Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "#A)Rank B)Name C)Artist D)Upload date  E)Views\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver--------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos--------------------------------------------\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "\n",
    "#length of table-----------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "\n",
    "#Scrape Rank-------------------------------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "rank=[]\n",
    "for i in range(tot_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[1]')\n",
    "    for element in w:\n",
    "        rank.append(element.text)\n",
    "print(rank)\n",
    "\n",
    "#Scrape Name-------------------------------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "name=[]\n",
    "for i in range(tot_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[2]')\n",
    "    for element in w:\n",
    "        name.append(element.text)\n",
    "print(name)\n",
    "\n",
    "#Scrape Artist-----------------------------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "artist=[]\n",
    "for i in range(tot_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[3]')\n",
    "    for element in w:\n",
    "        artist.append(element.text)\n",
    "print(artist)\n",
    "\n",
    "#Scrape Upload date------------------------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "upload_date=[]\n",
    "for i in range(tot_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[5]')\n",
    "    for element in w:\n",
    "        upload_date.append(element.text)\n",
    "print(upload_date)\n",
    "\n",
    "#Scrape Views in (billions)----------------------------------------------------------------------------------------------\n",
    "tot_len=30\n",
    "views=[]\n",
    "for i in range(tot_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[3]/div[5]/div[1]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[4]')\n",
    "    for element in w:\n",
    "        views.append(element.text)\n",
    "print(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b360c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "print(len(rank),len(name),len(artist),len(views),len(upload_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "819f3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Most viewed videos on YouTube from Wikipedia--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views(in billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[15]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[16]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[18]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[23]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Uptown Funk\"[24]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[25]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[27]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[32]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[33]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sugar\"[34]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[35]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[36]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[38]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Thinking Out Loud\"[39]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Dark Horse\"[41]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Faded\"[43]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Let Her Go\"[44]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Girls Like You\"[45]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Perfect\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Bailando\"[47]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Lean On\"[48]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[49]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[50]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                                  \"Bath Song\"[15]   \n",
       "4    5.                               \"Shape of You\"[16]   \n",
       "5    6.                              \"See You Again\"[18]   \n",
       "6    7.                \"Phonics Song with Two Words\"[23]   \n",
       "7    8.                                \"Uptown Funk\"[24]   \n",
       "8    9.  \"Learning Colors – Colorful Eggs on a Farm\"[25]   \n",
       "9   10.                          \"Wheels on the Bus\"[26]   \n",
       "10  11.                              \"Gangnam Style\"[27]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[32]   \n",
       "12  13.                             \"Dame Tu Cosita\"[33]   \n",
       "13  14.                                      \"Sugar\"[34]   \n",
       "14  15.                                       \"Roar\"[35]   \n",
       "15  16.                             \"Counting Stars\"[36]   \n",
       "16  17.                                     \"Axel F\"[37]   \n",
       "17  18.                                      \"Sorry\"[38]   \n",
       "18  19.                          \"Thinking Out Loud\"[39]   \n",
       "19  20.                        \"Baa Baa Black Sheep\"[40]   \n",
       "20  21.                                 \"Dark Horse\"[41]   \n",
       "21  22.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "22  23.                                      \"Faded\"[43]   \n",
       "23  24.                                 \"Let Her Go\"[44]   \n",
       "24  25.                             \"Girls Like You\"[45]   \n",
       "25  26.                                    \"Perfect\"[46]   \n",
       "26  27.                                   \"Bailando\"[47]   \n",
       "27  28.                                    \"Lean On\"[48]   \n",
       "28  29.          \"Humpty the train on a fruits ride\"[49]   \n",
       "29  30.                             \"Lakdi Ki Kathi\"[50]   \n",
       "\n",
       "                                           Artist        Upload Date  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                      Luis Fonsi   January 12, 2017   \n",
       "2                                     LooLoo Kids    October 8, 2016   \n",
       "3                      Cocomelon – Nursery Rhymes        May 2, 2018   \n",
       "4                                      Ed Sheeran   January 30, 2017   \n",
       "5                                     Wiz Khalifa      April 6, 2015   \n",
       "6                                       ChuChu TV      March 6, 2014   \n",
       "7                                     Mark Ronson  November 19, 2014   \n",
       "8                                     Miroshka TV  February 27, 2018   \n",
       "9                      Cocomelon – Nursery Rhymes       May 24, 2018   \n",
       "10                                            Psy      July 15, 2012   \n",
       "11                                     Get Movies   January 31, 2012   \n",
       "12                                      El Chombo      April 5, 2018   \n",
       "13                                       Maroon 5   January 14, 2015   \n",
       "14                                     Katy Perry  September 5, 2013   \n",
       "15                                    OneRepublic       May 31, 2013   \n",
       "16                                     Crazy Frog      June 16, 2009   \n",
       "17                                  Justin Bieber   October 22, 2015   \n",
       "18                                     Ed Sheeran    October 7, 2014   \n",
       "19                     Cocomelon – Nursery Rhymes      June 25, 2018   \n",
       "20                                     Katy Perry  February 20, 2014   \n",
       "21                                        Shakira       June 4, 2010   \n",
       "22                                    Alan Walker   December 3, 2015   \n",
       "23                                      Passenger      July 25, 2012   \n",
       "24                                       Maroon 5       May 31, 2018   \n",
       "25                                     Ed Sheeran   November 9, 2017   \n",
       "26                               Enrique Iglesias     April 11, 2014   \n",
       "27                                    Major Lazer     March 22, 2015   \n",
       "28  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "29                                   Jingle Toons      June 14, 2018   \n",
       "\n",
       "   Views(in billions)  \n",
       "0               11.98  \n",
       "1                8.04  \n",
       "2                6.56  \n",
       "3                5.88  \n",
       "4                5.87  \n",
       "5                5.73  \n",
       "6                5.07  \n",
       "7                4.78  \n",
       "8                4.76  \n",
       "9                4.76  \n",
       "10               4.63  \n",
       "11               4.52  \n",
       "12               4.17  \n",
       "13               3.80  \n",
       "14               3.70  \n",
       "15               3.70  \n",
       "16               3.66  \n",
       "17               3.61  \n",
       "18               3.53  \n",
       "19               3.46  \n",
       "20               3.42  \n",
       "21               3.39  \n",
       "22               3.38  \n",
       "23               3.36  \n",
       "24               3.35  \n",
       "25               3.33  \n",
       "26               3.31  \n",
       "27               3.30  \n",
       "28               3.26  \n",
       "29               3.24  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-------------------------- Most viewed videos on YouTube from Wikipedia--------------------------------------\")\n",
    "df1=pd.DataFrame({'Rank':rank,'Name':name,'Artist':artist,'Upload Date':upload_date,'Views(in billions)':views})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c1c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "#You need to find following details:  A)Match title(I.e. 1st ODI)   B)Series   C)Place   D)Date   E)Time\n",
    "#Note:- From bcci.tv home page you have reach to the international fixture page through code.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver----------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://www.bcci.tv/----------------------------------------------------------------------------\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "time.sleep(4)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "click1=driver.find_element(By.XPATH,\"/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a\")\n",
    "click1.click()\n",
    "time.sleep(4)\n",
    "\n",
    "click2=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[3]/div')\n",
    "click2.click()\n",
    "time.sleep(3)\n",
    "\n",
    "click3=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[3]/div/div[2]/div[3]')\n",
    "click3.click()\n",
    "time.sleep(3)\n",
    "\n",
    "click4=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[3]/div[2]')\n",
    "click4.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Scrape Match Title(I.e. 1st ODI)--------------------------------------------------------------------------------------\n",
    "match_title=[]\n",
    "try:\n",
    "    mt=driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "    for i in mt:\n",
    "        match_title.append(i.text[0:-1])\n",
    "except NOSuchElementException:\n",
    "    match_title.append(\"-\")\n",
    "print(match_title)\n",
    "\n",
    "#Scrape Series---------------------------------------------------------------------------------------------------------\n",
    "series=[]\n",
    "try:\n",
    "    se=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]')\n",
    "    for i in se:\n",
    "        series.append(i.text)\n",
    "except NOSuchElementException:\n",
    "    series.append(\"-\")\n",
    "print(series)\n",
    "\n",
    "#Scrape  Place----------------------------------------------------------------------------------------------------------\n",
    "place=[]\n",
    "try:\n",
    "    pl=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]')\n",
    "    for i in pl:\n",
    "        place.append(i.text[:-1])\n",
    "except NOSuchElementException:\n",
    "    place.append(\"-\")\n",
    "print(place)\n",
    "\n",
    "#Scrape Date------------------------------------------------------------------------------------------------------------\n",
    "date=[]\n",
    "try:\n",
    "    dt=driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]')\n",
    "    for i in dt:\n",
    "        date.append(i.text)\n",
    "except NOSuchElementException:\n",
    "    date.append(\"-\")\n",
    "print(date)\n",
    "\n",
    "#Scrape Time------------------------------------------------------------------------------------------------------------\n",
    "ti=[]\n",
    "try:\n",
    "    tt=driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]')\n",
    "    for i in tt:\n",
    "        ti.append(i.text)\n",
    "except NOSuchElementException:\n",
    "    ti.append(\"-\")\n",
    "print(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43482da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 9 9 9\n"
     ]
    }
   ],
   "source": [
    "print(len(series),len(match_title),len(place),len(date),len(ti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34c2bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------India’s international fixtures from bcci.tv.----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match title(I.e. 1st ODI)</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Barsapara Cricket Stadium</td>\n",
       "      <td>10 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Eden Gardens</td>\n",
       "      <td>12 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>SRI LANKA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Greenfield International Stadium</td>\n",
       "      <td>15 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Rajiv Gandhi International Stadium</td>\n",
       "      <td>18 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Shaheed Veer Narayan Singh International Crick...</td>\n",
       "      <td>21 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Holkar Cricket Stadium</td>\n",
       "      <td>24 JAN 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Wankhede Stadium</td>\n",
       "      <td>17 MAR 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stadium</td>\n",
       "      <td>19 MAR 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>MA Chidambaram Stadium</td>\n",
       "      <td>22 MAR 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match title(I.e. 1st ODI)                                        Series  \\\n",
       "0                  1st ODI     SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "1                  2nd ODI     SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "2                  3rd ODI     SRI LANKA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "3                  1st ODI   NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23   \n",
       "4                  2nd ODI   NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23   \n",
       "5                  3rd ODI   NEW ZEALAND TOUR OF INDIA ODI SERIES 2022-23   \n",
       "6                  1st ODI     AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "7                  2nd ODI     AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "8                  3rd ODI     AUSTRALIA TOUR OF INDIA ODI SERIES 2022-23   \n",
       "\n",
       "                                               Place         Date         Time  \n",
       "0                          Barsapara Cricket Stadium  10 JAN 2023  1:30 PM IST  \n",
       "1                                       Eden Gardens  12 JAN 2023  1:30 PM IST  \n",
       "2                   Greenfield International Stadium  15 JAN 2023  1:30 PM IST  \n",
       "3                 Rajiv Gandhi International Stadium  18 JAN 2023  1:30 PM IST  \n",
       "4  Shaheed Veer Narayan Singh International Crick...  21 JAN 2023  1:30 PM IST  \n",
       "5                             Holkar Cricket Stadium  24 JAN 2023  1:30 PM IST  \n",
       "6                                   Wankhede Stadium  17 MAR 2023  1:30 PM IST  \n",
       "7   Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stadium  19 MAR 2023  1:30 PM IST  \n",
       "8                             MA Chidambaram Stadium  22 MAR 2023  1:30 PM IST  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"------------------------India’s international fixtures from bcci.tv.----------------------------\")\n",
    "df2=pd.DataFrame({'Match title(I.e. 1st ODI)':match_title,'Series':series,'Place':place,'Date':date,'Time':ti})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d243102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/\n",
    "#You need to find following details:\n",
    "#A)Name  B)Description  Note:-From guru99 home page you have to reach to selenium exception handling page through code\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver----------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://www.guru99.com/----------------------------------------------------------------------------\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "time.sleep(4)\n",
    "\n",
    "click1=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a\")\n",
    "click1.click()\n",
    "time.sleep(6)\n",
    "\n",
    "click2=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]/a/strong\")\n",
    "click2.click()\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4 Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/\n",
    "#You have to find following details:\n",
    "#A) Rank   B)State   C)GSDP(18-19)-at current prices   D)GSDP(19-20)-at current prices  E)Share(18-19)  F)GDP($ billion)\n",
    "#Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver--------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to http://statisticstimes.com/--------------------------------------------------------------------------------\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "time.sleep(4)\n",
    "\n",
    "click1=driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]\")\n",
    "click1.click()\n",
    "time.sleep(6)\n",
    "\n",
    "click2=driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "click2.click()\n",
    "time.sleep(6)\n",
    "\n",
    "click3=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "click3.click()\n",
    "time.sleep(6)\n",
    "\n",
    "#Scrape Rank------------------------------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "ranks=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[1]')\n",
    "    for element in w:\n",
    "        ranks.append(element.text)\n",
    "print(ranks)\n",
    "\n",
    "#Scrape State------------------------------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "state=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[2]')\n",
    "    for element in w:\n",
    "        state.append(element.text)\n",
    "print(state)\n",
    "\n",
    "#Scrape GSDP(18-19)-at current prices--------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "gsdp18=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[4]')\n",
    "    for element in w:\n",
    "        gsdp18.append(element.text)\n",
    "print(gsdp18)\n",
    "\n",
    "#Scrape GSDP(19-20)-at current prices--------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "gsdp19=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[3]')\n",
    "    for element in w:\n",
    "        gsdp19.append(element.text)\n",
    "print(gsdp19)\n",
    "\n",
    "#Scrape Share(18-19)--------------------------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "share18=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[5]')\n",
    "    for element in w:\n",
    "        share18.append(element.text)\n",
    "print(share18)\n",
    "\n",
    "#Scrape GDP($ billion)------------------------------------------------------------------------------------------------------\n",
    "total_len=33\n",
    "GDP=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[6]')\n",
    "    for element in w:\n",
    "        GDP.append(element.text)\n",
    "print(GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e275078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n"
     ]
    }
   ],
   "source": [
    "print(len(ranks),len(state),len(gsdp18),len(gsdp19),len(share18),len(GDP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2e1b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Details of State-wise GDP of India from statisticstime.com---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19)-at current prices</th>\n",
       "      <th>GSDP(19-20)-at current prices</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP(18-19)-at current prices  \\\n",
       "0     1                Maharashtra                     2,632,792   \n",
       "1     2                 Tamil Nadu                     1,630,208   \n",
       "2     3              Uttar Pradesh                     1,584,764   \n",
       "3     4                    Gujarat                     1,502,899   \n",
       "4     5                  Karnataka                     1,493,127   \n",
       "5     6                West Bengal                     1,089,898   \n",
       "6     7                  Rajasthan                       942,586   \n",
       "7     8             Andhra Pradesh                       862,957   \n",
       "8     9                  Telangana                       861,031   \n",
       "9    10             Madhya Pradesh                       809,592   \n",
       "10   11                     Kerala                       781,653   \n",
       "11   12                      Delhi                       774,870   \n",
       "12   13                    Haryana                       734,163   \n",
       "13   14                      Bihar                       530,363   \n",
       "14   15                     Punjab                       526,376   \n",
       "15   16                     Odisha                       487,805   \n",
       "16   17                      Assam                       315,881   \n",
       "17   18               Chhattisgarh                       304,063   \n",
       "18   19                  Jharkhand                       297,204   \n",
       "19   20                Uttarakhand                       245,895   \n",
       "20   21            Jammu & Kashmir                       155,956   \n",
       "21   22           Himachal Pradesh                       153,845   \n",
       "22   23                        Goa                        73,170   \n",
       "23   24                    Tripura                        49,845   \n",
       "24   25                 Chandigarh                        42,114   \n",
       "25   26                 Puducherry                        34,433   \n",
       "26   27                  Meghalaya                        33,481   \n",
       "27   28                     Sikkim                        28,723   \n",
       "28   29                    Manipur                        27,870   \n",
       "29   30                   Nagaland                        27,283   \n",
       "30   31          Arunachal Pradesh                        24,603   \n",
       "31   32                    Mizoram                        22,287   \n",
       "32   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP(19-20)-at current prices Share(18-19) GDP($ billion)  \n",
       "0                              -       13.94%        399.921  \n",
       "1                      1,845,853        8.63%        247.629  \n",
       "2                      1,687,818        8.39%        240.726  \n",
       "3                              -        7.96%        228.290  \n",
       "4                      1,631,977        7.91%        226.806  \n",
       "5                      1,253,832        5.77%        165.556  \n",
       "6                      1,020,989        4.99%        143.179  \n",
       "7                        972,782        4.57%        131.083  \n",
       "8                        969,604        4.56%        130.791  \n",
       "9                        906,672        4.29%        122.977  \n",
       "10                             -        4.14%        118.733  \n",
       "11                       856,112        4.10%        117.703  \n",
       "12                       831,610        3.89%        111.519  \n",
       "13                       611,804        2.81%         80.562  \n",
       "14                       574,760        2.79%         79.957  \n",
       "15                       521,275        2.58%         74.098  \n",
       "16                             -        1.67%         47.982  \n",
       "17                       329,180        1.61%         46.187  \n",
       "18                       328,598        1.57%         45.145  \n",
       "19                             -        1.30%         37.351  \n",
       "20                             -        0.83%         23.690  \n",
       "21                       165,472        0.81%         23.369  \n",
       "22                        80,449        0.39%         11.115  \n",
       "23                        55,984        0.26%          7.571  \n",
       "24                             -        0.22%          6.397  \n",
       "25                        38,253        0.18%          5.230  \n",
       "26                        36,572        0.18%          5.086  \n",
       "27                        32,496        0.15%          4.363  \n",
       "28                        31,790        0.15%          4.233  \n",
       "29                             -        0.14%          4.144  \n",
       "30                             -        0.13%          3.737  \n",
       "31                        26,503        0.12%          3.385  \n",
       "32                             -            -              -  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4=pd.DataFrame({'Rank':ranks,'State':state,'GSDP(18-19)-at current prices':gsdp18,'GSDP(19-20)-at current prices':gsdp19,'Share(18-19)':share18,'GDP($ billion)':GDP})\n",
    "print('-----------------------Details of State-wise GDP of India from statisticstime.com---------------------------------------')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14d1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5 Scrape the details of trending repositories on Github.com. Url = https://github.com/\n",
    "#You have to find the following details: A)Repository title   B)Repository description   C)Contributors count   D)Language used\n",
    "#Note:From the home page you have to click on the trending option from Explore menu through code.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://github.com/--------------------------------------------------------------------------------------------\n",
    "driver.get(\"https://github.com/\")\n",
    "time.sleep(4)\n",
    "\n",
    "click1=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "click1.click()\n",
    "time.sleep(6)\n",
    "\n",
    "click2=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/ul[3]/li[3]/a\")\n",
    "click2.click()\n",
    "time.sleep(6)\n",
    "\n",
    "#Scrape URL's-------------------------------------------------------------------------------------------------------------\n",
    "repo_urls=[]\n",
    "try:\n",
    "    rurls=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h1/a')\n",
    "    for i in rurls:\n",
    "        repo_urls.append(i.get_attribute('href'))\n",
    "except NoSuchElementException:\n",
    "    repo_urls.append(\"-\")\n",
    "print(repo_urls)\n",
    "\n",
    "#Scrape Repository title---------------------------------------------------------------------------------------------------\n",
    "repo_title=[]\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(4)\n",
    "    try:\n",
    "        aa=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/main/div/div[1]/div/div/strong/a')\n",
    "        repo_title.append(aa.text)\n",
    "    except NoSuchElementException:\n",
    "        repo_title.append(\"-\")\n",
    "print(repo_title)\n",
    "\n",
    "#Scrape Repository description---------------------------------------------------------------------------------------------\n",
    "repo_desc=[]\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(4)\n",
    "    try:\n",
    "        aa=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[1]/div/p')\n",
    "        repo_desc.append(aa.text)\n",
    "    except NoSuchElementException:\n",
    "        repo_desc.append(\"-\")\n",
    "print(repo_desc)\n",
    "\n",
    "#Scrape Contributors count ------------------------------------------------------------------------------------------------\n",
    "contr=[]\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(6)\n",
    "    try:\n",
    "        cc=driver.find_element(By.XPATH,'//div[@class=\"BorderGrid-row\"][4]/div/h2/a/span')\n",
    "        contr.append(cc.text)\n",
    "    except NoSuchElementException:\n",
    "        contr.append(\"-\")\n",
    "print(contr)\n",
    "\n",
    "#Scrape Language used------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5cc76ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(repo_urls),len(repo_title),len(repo_desc),len(contr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fb9b2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zlib-searcher</td>\n",
       "      <td>search zlib/libgen index to get ipfs_cid.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open-Assistant</td>\n",
       "      <td>OpenAssistant is a chat-based assistant that u...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome-python</td>\n",
       "      <td>A curated list of awesome Python frameworks, l...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v2rayNG</td>\n",
       "      <td>A V2Ray client for Android, support Xray core ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nanoGPT</td>\n",
       "      <td>The simplest, fastest repository for training/...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>exploitation-course</td>\n",
       "      <td>Offensive Software Exploitation Course</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sre-interview-prep-guide</td>\n",
       "      <td>Site Reliability Engineer Interview Preparatio...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cs-video-courses</td>\n",
       "      <td>List of Computer Science courses with video le...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hackingtool</td>\n",
       "      <td>ALL IN ONE Hacking Tool For Hackers</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crux-top-lists</td>\n",
       "      <td>Downloadable snapshots of the Chrome Top Milli...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>termux-packages</td>\n",
       "      <td>A build system and primary set of packages for...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GCPSketchnote</td>\n",
       "      <td>If you are looking to become a Google Cloud En...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hyper</td>\n",
       "      <td>A terminal built on web technologies</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>alist</td>\n",
       "      <td>🗂️A file list program that supports multiple s...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>imgui</td>\n",
       "      <td>Dear ImGui: Bloat-free Graphical User interfac...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cal.com</td>\n",
       "      <td>Scheduling infrastructure for absolutely every...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>polars</td>\n",
       "      <td>Fast multi-threaded, hybrid-streaming DataFram...</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>docs</td>\n",
       "      <td>The open-source repo for docs.github.com</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt_index</td>\n",
       "      <td>An index created by GPT to organize external i...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OpenBBTerminal</td>\n",
       "      <td>Investment Research for Everyone, Anywhere.</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>docker-minecraft-server</td>\n",
       "      <td>Docker image that provides a Minecraft Server ...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>zustand</td>\n",
       "      <td>🐻 Bear necessities for state management in React</td>\n",
       "      <td>62.2k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>osv-scanner</td>\n",
       "      <td>Vulnerability scanner written in Go which uses...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Swiftfin</td>\n",
       "      <td>Native Jellyfin Client for iOS and tvOS</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>joplin</td>\n",
       "      <td>Joplin - an open source note taking and to-do ...</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Repository Title  \\\n",
       "0              zlib-searcher   \n",
       "1             Open-Assistant   \n",
       "2             awesome-python   \n",
       "3                    v2rayNG   \n",
       "4                    nanoGPT   \n",
       "5        exploitation-course   \n",
       "6   sre-interview-prep-guide   \n",
       "7           cs-video-courses   \n",
       "8                hackingtool   \n",
       "9             crux-top-lists   \n",
       "10           termux-packages   \n",
       "11             GCPSketchnote   \n",
       "12                     hyper   \n",
       "13                     alist   \n",
       "14                     imgui   \n",
       "15                   cal.com   \n",
       "16                    polars   \n",
       "17                      docs   \n",
       "18                 gpt_index   \n",
       "19            OpenBBTerminal   \n",
       "20   docker-minecraft-server   \n",
       "21                   zustand   \n",
       "22               osv-scanner   \n",
       "23                  Swiftfin   \n",
       "24                    joplin   \n",
       "\n",
       "                               Repository Description Contributors count  \n",
       "0           search zlib/libgen index to get ipfs_cid.                 10  \n",
       "1   OpenAssistant is a chat-based assistant that u...                 49  \n",
       "2   A curated list of awesome Python frameworks, l...                  -  \n",
       "3   A V2Ray client for Android, support Xray core ...                 23  \n",
       "4   The simplest, fastest repository for training/...                  -  \n",
       "5              Offensive Software Exploitation Course                  -  \n",
       "6   Site Reliability Engineer Interview Preparatio...                  -  \n",
       "7   List of Computer Science courses with video le...                 76  \n",
       "8                 ALL IN ONE Hacking Tool For Hackers                  -  \n",
       "9   Downloadable snapshots of the Chrome Top Milli...                  -  \n",
       "10  A build system and primary set of packages for...                  -  \n",
       "11  If you are looking to become a Google Cloud En...                  2  \n",
       "12               A terminal built on web technologies                274  \n",
       "13  🗂️A file list program that supports multiple s...                  -  \n",
       "14  Dear ImGui: Bloat-free Graphical User interfac...                  -  \n",
       "15  Scheduling infrastructure for absolutely every...                  -  \n",
       "16  Fast multi-threaded, hybrid-streaming DataFram...                368  \n",
       "17           The open-source repo for docs.github.com                  -  \n",
       "18  An index created by GPT to organize external i...                  8  \n",
       "19        Investment Research for Everyone, Anywhere.                158  \n",
       "20  Docker image that provides a Minecraft Server ...                  -  \n",
       "21   🐻 Bear necessities for state management in React              62.2k  \n",
       "22  Vulnerability scanner written in Go which uses...                 15  \n",
       "23            Native Jellyfin Client for iOS and tvOS                113  \n",
       "24  Joplin - an open source note taking and to-do ...                513  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5=pd.DataFrame({'Repository Title':repo_title,'Repository Description':repo_desc,'Contributors count':contr})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a9dcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6 Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "#You have to find the following details: A)Song name B)Artist name C)Last week rank D)Peak rank E)Weeks on board\n",
    "#Note: - From the home page you have to click on the charts option then hot 100-page link through code.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https:/www.billboard.com/---------------------------------------------------------------------------------------\n",
    "driver.get(\"https:/www.billboard.com/\")\n",
    "time.sleep(4)\n",
    "\n",
    "click1=driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]\")\n",
    "click1.click()\n",
    "time.sleep(6)\n",
    "\n",
    "click2=driver.find_element(By.XPATH,\"/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a\")\n",
    "click2.click()\n",
    "time.sleep(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b14c1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7)Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/\n",
    "#You have to find the following details: A)Name B)Designation C)Company D)Skills they hire for E)Location\n",
    "#Note:From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://www.naukri.com/--------------------------------------------------------------------------------------------\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "time.sleep(4)\n",
    "\n",
    "#Enter Data Science \n",
    "des=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "des.send_keys('Data Science')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "#Scrape URL's-------------------------------------------------------------------------------------------------------------\n",
    "urls=[]\n",
    "try:\n",
    "    ur=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "    for i in ur:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "except NoSuchElementException:\n",
    "        urls.append(\"-\")\n",
    "print(urls)\n",
    "\n",
    "#Scrape Name----------------------------------------------------------------------------------------------------------------------\n",
    "name=[]\n",
    "try:\n",
    "    nn=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "    for i in nn:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        name.append(\"-\")\n",
    "print(name)\n",
    "\n",
    "#Scrape Designation---------------------------------------------------------------------------------------------------------------- \n",
    "desig=[]\n",
    "try:\n",
    "    dg=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "    for i in dg:\n",
    "        desig.append(i.text.split('-')[0])\n",
    "except NoSuchElementException:\n",
    "        desig.append(\"-\")\n",
    "print(desig)\n",
    "\n",
    "#Scrape Company--------------------------------------------------------------------------------------------------------------------\n",
    "comp=[]\n",
    "try:\n",
    "    co=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in co:\n",
    "        comp.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        comp.append(\"-\")\n",
    "print(comp)\n",
    "\n",
    "#Scrape Skills they hire for--------------------------------------------------------------------------------------------------------\n",
    "skills=[]\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(6)\n",
    "    try:\n",
    "        sk=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[2]/div[4]')\n",
    "        skills.append(sk.text)\n",
    "    except NoSuchElementException:\n",
    "        skills.append(\"-\")\n",
    "print(skills)\n",
    "\n",
    "#Scrape Location----------------------------------------------------------------------------------------------------------------\n",
    "loc=[]\n",
    "try:\n",
    "    lo=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "    for i in lo:\n",
    "        loc.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        loc.append(\"-\")\n",
    "print(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9d95350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(urls),len(name),len(comp),len(loc),len(desig),len(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf155b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Details of Data science recruiters from naukri.com-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills they hire for</th>\n",
       "      <th>Loaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python - Data Science Analyst</td>\n",
       "      <td>Python</td>\n",
       "      <td>Infosys</td>\n",
       "      <td>-</td>\n",
       "      <td>Bhubaneswar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technology Analyst - Data Science / Machine Le...</td>\n",
       "      <td>Technology Analyst</td>\n",
       "      <td>Infosys</td>\n",
       "      <td>-</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst - Data Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Awign Enterprises</td>\n",
       "      <td>Key Skills\\nData ScienceData PipelineDashboard...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science / Lead Data Science / Analytics w...</td>\n",
       "      <td>Data Science / Lead Data Science / Analytics w...</td>\n",
       "      <td>Premier Consultants</td>\n",
       "      <td>Key Skills\\nData Science\\nLendingdata analytic...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consultant/Senior Manager - Analytics/Data Sci...</td>\n",
       "      <td>Consultant/Senior Manager</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>Key Skills\\nData Science\\nSASPPTMachine Learni...</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACN - Applied Intelligence - Finance - Data Sc...</td>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Key Skills\\nComputer scienceData analysisHealt...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACN - Applied Intelligence - Finance - Data Sc...</td>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Key Skills\\nData analysisMachine learningHealt...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager- Data Science</td>\n",
       "      <td>Manager</td>\n",
       "      <td>Tredence</td>\n",
       "      <td>Key Skills\\ndata scienceMachine LearningPython...</td>\n",
       "      <td>Hybrid - Pune, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>EY Global Delivery Services India Llp</td>\n",
       "      <td>Key Skills\\nFMCG MarketingRetail AnalyticsInte...</td>\n",
       "      <td>Hybrid - Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SENIOR MANAGER I, DATA SCIENCE</td>\n",
       "      <td>SENIOR MANAGER I, DATA SCIENCE</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Key Skills\\nAutomationData analysisProduction ...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Manager II, Data Science</td>\n",
       "      <td>Senior Manager II, Data Science</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Key Skills\\nData analysisData managementCoding...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Science Professional</td>\n",
       "      <td>Data Science Professional</td>\n",
       "      <td>Infosys</td>\n",
       "      <td>-</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Analyst - Data Science</td>\n",
       "      <td>Senior Analyst</td>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>Key Skills\\ndata scienceProcess documentationS...</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Delivery Head - Analytics &amp; Data Science</td>\n",
       "      <td>Delivery Head</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>Key Skills\\nAnalytics\\nData ScienceMachine Lea...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Associate Principal - Data Science</td>\n",
       "      <td>Associate Principal</td>\n",
       "      <td>Themathcompany</td>\n",
       "      <td>Key Skills\\nSASAssociate Principaldata science...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer/data Science Consultant</td>\n",
       "      <td>Data Engineer/data Science Consultant</td>\n",
       "      <td>Career Infosystem</td>\n",
       "      <td>Key Skills\\nData Science\\nBusiness Intelligenc...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lead Data Science Engineer - Python/Machine Le...</td>\n",
       "      <td>Lead Data Science Engineer</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>Key Skills\\nPython\\nData ScienceMySQLData Mana...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Science - Contract To Hire</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Successr Hr Tech</td>\n",
       "      <td>Key Skills\\nMatLab\\ndata analysisdata miningNo...</td>\n",
       "      <td>Temp. WFH - Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data science Architect</td>\n",
       "      <td>Data science Architect</td>\n",
       "      <td>Hucon</td>\n",
       "      <td>Key Skills\\nData science\\nTensorflowLogistic R...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Science| SQL| Python| Visualization - Ana...</td>\n",
       "      <td>Data Science| SQL| Python| Visualization</td>\n",
       "      <td>Samaira Hr Solutions</td>\n",
       "      <td>Key Skills\\npythonData Modelingsql\\nData Scien...</td>\n",
       "      <td>Hybrid - Pune, Bangalore/Bengaluru, Mumbai (Al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                       Python - Data Science Analyst   \n",
       "1   Technology Analyst - Data Science / Machine Le...   \n",
       "2                         Data Analyst - Data Science   \n",
       "3   Data Science / Lead Data Science / Analytics w...   \n",
       "4   Consultant/Senior Manager - Analytics/Data Sci...   \n",
       "5   ACN - Applied Intelligence - Finance - Data Sc...   \n",
       "6   ACN - Applied Intelligence - Finance - Data Sc...   \n",
       "7                               Manager- Data Science   \n",
       "8                                Data Science Analyst   \n",
       "9                      SENIOR MANAGER I, DATA SCIENCE   \n",
       "10                    Senior Manager II, Data Science   \n",
       "11                          Data Science Professional   \n",
       "12                      Senior Analyst - Data Science   \n",
       "13           Delivery Head - Analytics & Data Science   \n",
       "14                 Associate Principal - Data Science   \n",
       "15              Data Engineer/data Science Consultant   \n",
       "16  Lead Data Science Engineer - Python/Machine Le...   \n",
       "17                    Data Science - Contract To Hire   \n",
       "18                             Data science Architect   \n",
       "19  Data Science| SQL| Python| Visualization - Ana...   \n",
       "\n",
       "                                          Designation  \\\n",
       "0                                             Python    \n",
       "1                                 Technology Analyst    \n",
       "2                                       Data Analyst    \n",
       "3   Data Science / Lead Data Science / Analytics w...   \n",
       "4                          Consultant/Senior Manager    \n",
       "5                                                ACN    \n",
       "6                                                ACN    \n",
       "7                                             Manager   \n",
       "8                                Data Science Analyst   \n",
       "9                      SENIOR MANAGER I, DATA SCIENCE   \n",
       "10                    Senior Manager II, Data Science   \n",
       "11                          Data Science Professional   \n",
       "12                                    Senior Analyst    \n",
       "13                                     Delivery Head    \n",
       "14                               Associate Principal    \n",
       "15              Data Engineer/data Science Consultant   \n",
       "16                        Lead Data Science Engineer    \n",
       "17                                      Data Science    \n",
       "18                             Data science Architect   \n",
       "19          Data Science| SQL| Python| Visualization    \n",
       "\n",
       "                                  Company  \\\n",
       "0                                 Infosys   \n",
       "1                                 Infosys   \n",
       "2                       Awign Enterprises   \n",
       "3                     Premier Consultants   \n",
       "4               Huquo Consulting Pvt. Ltd   \n",
       "5                               Accenture   \n",
       "6                               Accenture   \n",
       "7                                Tredence   \n",
       "8   EY Global Delivery Services India Llp   \n",
       "9                                 Walmart   \n",
       "10                                Walmart   \n",
       "11                                Infosys   \n",
       "12                        Tiger Analytics   \n",
       "13                         Okda Solutions   \n",
       "14                         Themathcompany   \n",
       "15                      Career Infosystem   \n",
       "16              Huquo Consulting Pvt. Ltd   \n",
       "17                       Successr Hr Tech   \n",
       "18                                  Hucon   \n",
       "19                   Samaira Hr Solutions   \n",
       "\n",
       "                                 Skills they hire for  \\\n",
       "0                                                   -   \n",
       "1                                                   -   \n",
       "2   Key Skills\\nData ScienceData PipelineDashboard...   \n",
       "3   Key Skills\\nData Science\\nLendingdata analytic...   \n",
       "4   Key Skills\\nData Science\\nSASPPTMachine Learni...   \n",
       "5   Key Skills\\nComputer scienceData analysisHealt...   \n",
       "6   Key Skills\\nData analysisMachine learningHealt...   \n",
       "7   Key Skills\\ndata scienceMachine LearningPython...   \n",
       "8   Key Skills\\nFMCG MarketingRetail AnalyticsInte...   \n",
       "9   Key Skills\\nAutomationData analysisProduction ...   \n",
       "10  Key Skills\\nData analysisData managementCoding...   \n",
       "11                                                  -   \n",
       "12  Key Skills\\ndata scienceProcess documentationS...   \n",
       "13  Key Skills\\nAnalytics\\nData ScienceMachine Lea...   \n",
       "14  Key Skills\\nSASAssociate Principaldata science...   \n",
       "15  Key Skills\\nData Science\\nBusiness Intelligenc...   \n",
       "16  Key Skills\\nPython\\nData ScienceMySQLData Mana...   \n",
       "17  Key Skills\\nMatLab\\ndata analysisdata miningNo...   \n",
       "18  Key Skills\\nData science\\nTensorflowLogistic R...   \n",
       "19  Key Skills\\npythonData Modelingsql\\nData Scien...   \n",
       "\n",
       "                                             Loaction  \n",
       "0                                         Bhubaneswar  \n",
       "1                                 Bangalore/Bengaluru  \n",
       "2                                 Bangalore/Bengaluru  \n",
       "3                                 Bangalore/Bengaluru  \n",
       "4               Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "5                                 Bangalore/Bengaluru  \n",
       "6                                 Bangalore/Bengaluru  \n",
       "7     Hybrid - Pune, Bangalore/Bengaluru, Delhi / NCR  \n",
       "8      Hybrid - Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "9                                 Bangalore/Bengaluru  \n",
       "10                                Bangalore/Bengaluru  \n",
       "11                                Bangalore/Bengaluru  \n",
       "12                                            Chennai  \n",
       "13                                   Gurgaon/Gurugram  \n",
       "14                                Bangalore/Bengaluru  \n",
       "15                                Bangalore/Bengaluru  \n",
       "16  Hyderabad/Secunderabad, Pune, Chennai, Gurgaon...  \n",
       "17                    Temp. WFH - Bangalore/Bengaluru  \n",
       "18                                Bangalore/Bengaluru  \n",
       "19  Hybrid - Pune, Bangalore/Bengaluru, Mumbai (Al...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7=pd.DataFrame({'Name':name,'Designation':desig,'Company':comp,'Skills they hire for':skills,'Loaction':loc})\n",
    "print(\"-------------------------Details of Data science recruiters from naukri.com-------------------------------------\")\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecc8841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8) Scrape the details of Highest selling novels. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey- compare/\n",
    "#You have to find the following details: A)Book name  B)Author name  C)Volumes sold  D)Publisher  E)Genre\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare----\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "time.sleep(4)\n",
    "\n",
    "#Scrape Book name--------------------------------------------------------------------------\n",
    "total_len=100\n",
    "b_name=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[2]')\n",
    "    for element in w:\n",
    "        b_name.append(element.text)\n",
    "print(b_name)\n",
    "\n",
    "#Scrape Author name--------------------------------------------------------------------------\n",
    "total_len=100\n",
    "auth_name=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[3]')\n",
    "    for element in w:\n",
    "        auth_name.append(element.text)\n",
    "print(auth_name)\n",
    "\n",
    "#Scrape Voulmes sold--------------------------------------------------------------------------\n",
    "total_len=100\n",
    "volum=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[4]')\n",
    "    for element in w:\n",
    "        volum.append(element.text)\n",
    "print(volum)\n",
    "\n",
    "#Scrape Publisher------------------------------------------------------------------------------\n",
    "total_len=100\n",
    "publish=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[5]')\n",
    "    for element in w:\n",
    "        publish.append(element.text)\n",
    "print(publish)\n",
    "\n",
    "#Scrape Genre-----------------------------------------------------------------------------------\n",
    "total_len=100\n",
    "genre=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr[' + \n",
    "        str(i) + ']/td[6]')\n",
    "    for element in w:\n",
    "        genre.append(element.text)\n",
    "print(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34690c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(b_name),len(auth_name),len(volum),len(publish),len(genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3c62bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Details of Highest selling novels--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book name</th>\n",
       "      <th>Author name</th>\n",
       "      <th>Volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book name       Author name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8=pd.DataFrame({'Book name':b_name,'Author name':auth_name,'Volumes sold':volum,'Publisher':publish,'Genre':genre})\n",
    "print(\"----------------------------Details of Highest selling novels--------------------------------------------------\")\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead2f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9)Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "#You have to find the following details:A)Name  B)Year span C)Genre  D)Run time  E)Ratings   F)Votes\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to  https://www.imdb.com/list/ls095964455/------------------------------------------------------------------------\n",
    "driver.get(\" https://www.imdb.com/list/ls095964455/\")\n",
    "time.sleep(4)\n",
    "\n",
    "#Scrape Name---------------------------------------------------------------------------------------------------------------\n",
    "Name=[]\n",
    "try:\n",
    "    na=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]')\n",
    "    for i in na:\n",
    "        Name.append(i.text.split('(')[0])\n",
    "except NoSuchElementException:\n",
    "        Name.append(\"-\")\n",
    "print(Name)\n",
    "\n",
    "#Scrape Year span----------------------------------------------------------------------------------------------------------\n",
    "year=[]\n",
    "try:\n",
    "    ye=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]')\n",
    "    for i in ye:\n",
    "        year.append(i.text.split('(')[1])\n",
    "except NoSuchElementException:\n",
    "        year.append(\"-\")\n",
    "print(year)\n",
    "\n",
    "#Scrape Genre--------------------------------------------------------------------------------------------------------------\n",
    "genre=[]\n",
    "try:\n",
    "    ge=driver.find_elements(By.XPATH,'//span[@class=\"certificate\"]')\n",
    "    for i in ge:\n",
    "        genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        genre.append(\"-\")\n",
    "print(genre)\n",
    "\n",
    "#Scrape Run time-----------------------------------------------------------------------------------------------------------\n",
    "run_ti=[]\n",
    "try:\n",
    "    rt=driver.find_elements(By.XPATH,'//span[@class=\"runtime\"]')\n",
    "    for i in rt:\n",
    "        run_ti.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        run_ti.append(\"-\")\n",
    "print(run_ti)\n",
    "\n",
    "#Scrape Ratings-----------------------------------------------------------------------------------------------------------\n",
    "rating=[]\n",
    "try:\n",
    "    rt=driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-star small\"]')\n",
    "    for i in rt:\n",
    "        rating.append(i.text)\n",
    "except NoSuchElementException:\n",
    "        rating.append(\"-\")\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8a6a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Name),len(year),len(genre),len(run_ti),len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "103a5866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Details of most watched tv series of all time from imdb.com.--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Game of Thrones</td>\n",
       "      <td>2011–2019)</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Stranger Things</td>\n",
       "      <td>2016– )</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. The Walking Dead</td>\n",
       "      <td>2010–2022)</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. 13 Reasons Why</td>\n",
       "      <td>2017–2020)</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. The 100</td>\n",
       "      <td>2014–2020)</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96. Reign</td>\n",
       "      <td>2013–2017)</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97. A Series of Unfortunate Events</td>\n",
       "      <td>2017–2019)</td>\n",
       "      <td>TV-PG</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98. Criminal Minds</td>\n",
       "      <td>2005– )</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99. Scream: The TV Series</td>\n",
       "      <td>2015–2019)</td>\n",
       "      <td>TV-14</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100. The Haunting of Hill House</td>\n",
       "      <td>2018)</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Name   Year span  Genre Run time Ratings\n",
       "0                   1. Game of Thrones   2011–2019)  TV-MA   57 min     9.2\n",
       "1                   2. Stranger Things      2016– )  TV-14   51 min     8.7\n",
       "2                  3. The Walking Dead   2010–2022)  TV-MA   44 min     8.1\n",
       "3                    4. 13 Reasons Why   2017–2020)  TV-MA   60 min     7.5\n",
       "4                           5. The 100   2014–2020)  TV-14   43 min     7.6\n",
       "..                                  ...         ...    ...      ...     ...\n",
       "95                           96. Reign   2013–2017)  TV-14   42 min     7.4\n",
       "96  97. A Series of Unfortunate Events   2017–2019)  TV-PG   50 min     7.8\n",
       "97                  98. Criminal Minds      2005– )  TV-14   42 min     8.1\n",
       "98           99. Scream: The TV Series   2015–2019)  TV-14   45 min     7.1\n",
       "99     100. The Haunting of Hill House        2018)  TV-MA  572 min     8.6\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9=pd.DataFrame({'Name':Name,'Year span':year,'Genre':genre,'Run time':run_ti,'Ratings':rating})\n",
    "print(\"--------------------Details of most watched tv series of all time from imdb.com.--------------------------------------\")\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb3f1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10) Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "#You have to find the following details: A)Dataset name B)Data type C)Task D)Attribute type E)No of instances F)No of attribute G)Year\n",
    "#Note:from the home page you have to go to the ShowAllDataset page through code.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "#connect to the web driver-------------------------------------------------------------------------------------------------\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Asus i3\\Downloads\\chromedriver_win32.chromedriver.exe\")\n",
    "\n",
    "#connect to   https://archive.ics.uci.edu/---------------------------------------------------------------------------------\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "time.sleep(4)\n",
    "\n",
    "se1=driver.find_element(By.XPATH,\"/html/body/table[2]/tbody/tr/td/span/b/a\")\n",
    "se1.click()\n",
    "\n",
    "#Scrape Dataset name-------------------------------------------------------------------------\n",
    "total_len=622\n",
    "ds_name=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[1]/table/tbody/tr/td[2]/p/b/a')\n",
    "    for element in w:\n",
    "        ds_name.append(element.text)\n",
    "print(ds_name)\n",
    "\n",
    "#Scrape Data Type----------------------------------------------------------------------------\n",
    "total_len=622\n",
    "dataty=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[2]/p')\n",
    "    for element in w:\n",
    "        dataty.append(element.text)\n",
    "print(dataty)\n",
    "\n",
    "#Scrape Task------------------------------------------------------------------------\n",
    "total_len=622\n",
    "task=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[3]/p')\n",
    "    for element in w:\n",
    "        task.append(element.text)\n",
    "print(task)\n",
    "\n",
    "#Scrape Attribute type------------------------------------------------------------------------\n",
    "total_len=622\n",
    "attri=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[4]/p')\n",
    "    for element in w:\n",
    "        attri.append(element.text)\n",
    "print(attri)\n",
    "\n",
    "#Scrape No of instances------------------------------------------------------------------------\n",
    "total_len=622\n",
    "insta=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[5]/p')\n",
    "    for element in w:\n",
    "        insta.append(element.text)\n",
    "print(insta)\n",
    "\n",
    "#Scrape No of attribute------------------------------------------------------------------------\n",
    "total_len=622\n",
    "att=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[6]/p')\n",
    "    for element in w:\n",
    "        att.append(element.text)\n",
    "print(att)\n",
    "\n",
    "#Scrape year------------------------------------------------------------------------\n",
    "total_len=622\n",
    "year=[]\n",
    "for i in range(total_len+1):\n",
    "    w = driver.find_elements(By.XPATH, \n",
    "        '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr[' + \n",
    "        str(i) + ']/td[7]/p')\n",
    "    for element in w:\n",
    "        year.append(element.text)\n",
    "print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ff3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621 622 622 622 622 622 622\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_name),len(dataty),len(task),len(attri),len(insta),len(att),len(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b58f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Details of Datasets from UCI machine learning repositories-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute type</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Types</td>\n",
       "      <td>Default Task</td>\n",
       "      <td>Attribute Types</td>\n",
       "      <td># Instances</td>\n",
       "      <td># Attributes</td>\n",
       "      <td>Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Integer</td>\n",
       "      <td>35040</td>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>75840</td>\n",
       "      <td>525</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>400</td>\n",
       "      <td>50</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td></td>\n",
       "      <td>Classification</td>\n",
       "      <td></td>\n",
       "      <td>1014</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>10129</td>\n",
       "      <td>16</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Data type                  Task  \\\n",
       "0                    Data Types          Default Task   \n",
       "1                 Multivariate        Classification    \n",
       "2                 Multivariate        Classification    \n",
       "3                 Multivariate        Classification    \n",
       "4                                Recommender-Systems    \n",
       "..                          ...                   ...   \n",
       "617               Multivariate            Regression    \n",
       "618               Multivariate        Classification    \n",
       "619               Multivariate        Classification    \n",
       "620                                   Classification    \n",
       "621  Multivariate, Time-Series        Classification    \n",
       "\n",
       "                  Attribute type No of instances No of attribute   Year  \n",
       "0                Attribute Types     # Instances    # Attributes   Year  \n",
       "1    Categorical, Integer, Real            4177               8   1995   \n",
       "2          Categorical, Integer           48842              14   1996   \n",
       "3    Categorical, Integer, Real             798              38          \n",
       "4                   Categorical           37711             294   1998   \n",
       "..                           ...             ...             ...    ...  \n",
       "617                     Integer           35040              11   2020   \n",
       "618               Integer, Real           75840             525   2020   \n",
       "619               Integer, Real             400              50   2020   \n",
       "620                                        1014               7   2020   \n",
       "621                        Real           10129              16   2021   \n",
       "\n",
       "[622 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10=pd.DataFrame({'Data type':dataty,'Task':task,'Attribute type':attri,'No of instances':insta,'No of attribute':att,'Year':year})\n",
    "print(\"-------------------Details of Datasets from UCI machine learning repositories-----------------------------\")\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044025b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
